{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c865959-49e0-4bb7-a730-dfaa6dcaab87",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention is an important part of all Transformer-based models.\n",
    "This tutorial will show how to write it and how to then optimize it.\n",
    "\n",
    "Let's start with an overview on BERT model architecture, where the main building block is Transfomrer Encoder, which in turn has the main block Multi-Head Attention (MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e2e5d-4da4-4992-b6fc-b5bbdb0ac110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model architecture and size\n",
    "from IPython.display import Image\n",
    "Image(filename='bert_arch_table.jpg', width=400)\n",
    "# add length of seq x number of heads = 64 x 16 = 1024 in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ff33f-c0f0-4191-a6f2-77bd7562b2bd",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1ad4a-6178-4e78-84ce-94baff849c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open(device_id)\n",
    "from ttnn import transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5417e9-6dbf-4072-959d-20acff554574",
   "metadata": {},
   "source": [
    "## Enable program cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec920b5-42ab-463c-bae9-2b81a9bd144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.enable_program_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fccb3-94f4-4b59-ba74-f4bdeda3331e",
   "metadata": {},
   "source": [
    "### MHA overview: Tensor processing and shaping\n",
    "- First step is linear op (3x) between the input embeddings and each of the Q,K,V model weights.\n",
    "- Second step, splitting the Q, K, V along # of heads.\n",
    "- Third, running the attention score block.\n",
    "- Fourth step is concatenating the per head tensors into a merged one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f615d9-8fef-4a93-b5f4-d74423e38f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA overview: Tensor processing and shaping\n",
    "Image(filename='bert_tensor_shape.jpg', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d958d68-bb63-4f32-a895-63e90f1a20c7",
   "metadata": {},
   "source": [
    "### MHA OPs flow in TT-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f2c8c-88d4-4594-b356-650eb41d76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='bert_ops_conventional.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e95631-5163-4079-bb94-110d15252326",
   "metadata": {},
   "source": [
    "## Write Multi-Head Attention using ttnn\n",
    "\n",
    "Multi-head can be implemented in `torch` using just 6 operations:\n",
    "1. `torch.matmul`\n",
    "2. `torch.add` (bias)\n",
    "3. `torch.reshape`\n",
    "4. `torch.permute`\n",
    "5. `torch.mul` (scale)\n",
    "6. `torch.softmax`\n",
    "\n",
    "`ttnn` provides the exact same APIs to do that and therefore multi-head attention can be implemented as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b5aef4-62ad-4df7-a375-1c658402cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(\n",
    "    hidden_states,\n",
    "    query_weight,\n",
    "    query_bias,\n",
    "    key_weight,\n",
    "    key_bias,\n",
    "    value_weight,\n",
    "    value_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    *,\n",
    "    num_heads,\n",
    "):\n",
    "    batch_size, sequence_size, hidden_size = hidden_states.shape\n",
    "    head_size = hidden_size // num_heads\n",
    "\n",
    "    query = hidden_states @ query_weight\n",
    "    query = query + query_bias\n",
    "    query = ttnn.to_layout(query, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    query = ttnn.reshape(query, (batch_size, sequence_size, num_heads, head_size))\n",
    "    query = ttnn.to_layout(query, layout=ttnn.TILE_LAYOUT)\n",
    "    query = ttnn.permute(query, (0, 2, 1, 3))\n",
    "\n",
    "    key = hidden_states @ key_weight\n",
    "    key = key + key_bias\n",
    "    key = ttnn.to_layout(key, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    key = ttnn.reshape(key, (batch_size, sequence_size, num_heads, head_size))\n",
    "    key = ttnn.to_layout(key, layout=ttnn.TILE_LAYOUT)\n",
    "    key = ttnn.permute(key, (0, 2, 3, 1))\n",
    "\n",
    "    value = hidden_states @ value_weight\n",
    "    value = value + value_bias\n",
    "    value = ttnn.to_layout(value, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    value = ttnn.reshape(value, (batch_size, sequence_size, num_heads, head_size))\n",
    "    value = ttnn.to_layout(value, layout=ttnn.TILE_LAYOUT)\n",
    "    value = ttnn.permute(value, (0, 2, 1, 3))\n",
    "\n",
    "    attention_scores = query @ key\n",
    "    attention_scores = attention_scores * (1 / (head_size**0.5))\n",
    "    attention_probs = ttnn.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    context_layer = attention_probs @ value\n",
    "    context_layer = ttnn.permute(context_layer, (0, 2, 1, 3))\n",
    "    context_layer = ttnn.to_layout(context_layer, layout=ttnn.ROW_MAJOR_LAYOUT)\n",
    "    context_layer = ttnn.reshape(context_layer, (batch_size, sequence_size, hidden_size))\n",
    "    context_layer = ttnn.to_layout(context_layer, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "    self_output = context_layer @ output_weight\n",
    "    self_output = self_output + output_bias\n",
    "\n",
    "    return self_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f695b-0f84-42ba-a0bf-dba3d644f084",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ec9a4-5b93-44d6-85c6-325dbe0dcf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "sequence_size = 384\n",
    "num_heads = 16\n",
    "head_size = 64\n",
    "hidden_size = num_heads * head_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840050d9-a775-45ea-ac26-2c54ba15d722",
   "metadata": {},
   "source": [
    "## Initialize activations and weights using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5728477-2971-40b0-960d-acdc48a82b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_hidden_states = torch.randn((batch_size, sequence_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_query_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_query_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_key_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_key_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_value_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_value_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)\n",
    "torch_output_weight = torch.randn((hidden_size, hidden_size), dtype=torch.bfloat16)\n",
    "torch_output_bias = torch.randn((hidden_size,), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75894974-02fe-4056-afbe-d4536188f727",
   "metadata": {},
   "source": [
    "## Convert activations and weights to ttnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e016366-12b5-46f9-ba11-8b9124314f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = ttnn.from_torch(torch_hidden_states, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "query_weight = ttnn.from_torch(torch_query_weight, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "query_bias = ttnn.from_torch(torch_query_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "key_weight = ttnn.from_torch(torch_key_weight, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "key_bias = ttnn.from_torch(torch_key_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "value_weight = ttnn.from_torch(torch_value_weight, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "value_bias = ttnn.from_torch(torch_value_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "output_weight = ttnn.from_torch(torch_output_weight, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "output_bias = ttnn.from_torch(torch_output_bias, layout=ttnn.TILE_LAYOUT, device=device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07001e6-2f14-4827-9558-5c076e22d3dc",
   "metadata": {},
   "source": [
    "## Run the first iteration of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc744239-9726-4dc5-9021-d7f83994469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "multi_head_attention(\n",
    "    hidden_states,\n",
    "    query_weight,\n",
    "    query_bias,\n",
    "    key_weight,\n",
    "    key_bias,\n",
    "    value_weight,\n",
    "    value_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "end = time.time()\n",
    "duration = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce82f9d-5974-4def-8652-2a9389e76332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Multi-head attention ran in {duration} seconds for the first iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb20579-1090-47f7-bdfa-9a2362406fe2",
   "metadata": {},
   "source": [
    "## Run a subsequent iteration of Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e0304-548f-4474-b56f-49ed3761914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "output = multi_head_attention(\n",
    "    hidden_states,\n",
    "    query_weight,\n",
    "    query_bias,\n",
    "    key_weight,\n",
    "    key_bias,\n",
    "    value_weight,\n",
    "    value_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "end = time.time()\n",
    "duration = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecba30e-4fda-40d3-aa97-d19c73ac786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81aa66-e399-4092-b4c7-8a8250af508a",
   "metadata": {},
   "source": [
    "## Write optimized version of Multi-Head Attention\n",
    "\n",
    "Optimized version of the multi-head attention can be written by:\n",
    "- Tilizing all of the tensors ahead of time\n",
    "- Using more performant matmuls that fuse bias and specify the number of cores they execute on (The next 2 schematics are explaining the fused ops mapping to the conventional flow)\n",
    "- Putting every tensor into L1\n",
    "- Using bfloat8_b data_type\n",
    "- Using custom `transformer` operations instead of `ttnn.permute` and `ttnn.reshape`\n",
    "\n",
    "`ttnn.deallocate` calls are needed because otherwise, the cores on the device will run out of the L1 memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c647d1-20cd-4217-80ec-b8a7398c91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='bert_ops_optim_qkv_fuse.jpg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b0629-ad07-4f03-8b96-b5081ea3a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='bert_ops_optim_attention.jpg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc116b49-d306-4b30-9b63-62a34d2341c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_multi_head_attention(\n",
    "    hidden_states,\n",
    "    fused_qkv_weight,\n",
    "    fused_qkv_bias,\n",
    "    self_output_weight,\n",
    "    self_output_bias,\n",
    "    *,\n",
    "    num_heads,\n",
    "    num_cores_x=12,\n",
    "):\n",
    "    batch_size, _, hidden_size = hidden_states.shape\n",
    "    head_size = hidden_size // num_heads\n",
    "    \n",
    "    hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)\n",
    "\n",
    "    fused_qkv_output = ttnn.linear(\n",
    "        hidden_states,\n",
    "        fused_qkv_weight,\n",
    "        bias=fused_qkv_bias,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "        dtype=ttnn.bfloat8_b,\n",
    "        core_grid=(batch_size, num_cores_x),\n",
    "    )\n",
    "\n",
    "    (\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "    ) = ttnn.transformer.split_query_key_value_and_split_heads(\n",
    "        fused_qkv_output,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "        num_heads=num_heads,\n",
    "    )\n",
    "    ttnn.deallocate(fused_qkv_output)\n",
    "\n",
    "    attention_scores = ttnn.matmul(\n",
    "        query,\n",
    "        key,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "        dtype=ttnn.bfloat16,\n",
    "        core_grid=(batch_size, num_cores_x),\n",
    "    )\n",
    "    ttnn.deallocate(query)\n",
    "    ttnn.deallocate(key)\n",
    "\n",
    "    attention_probs = ttnn.transformer.attention_softmax(attention_scores, attention_mask=None, head_size=head_size)\n",
    "\n",
    "    context_layer = ttnn.matmul(\n",
    "        attention_probs,\n",
    "        value,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "        dtype=ttnn.bfloat8_b,\n",
    "        core_grid=(batch_size, num_cores_x),\n",
    "    )\n",
    "    ttnn.deallocate(attention_probs)\n",
    "\n",
    "    context_layer = ttnn.transformer.concatenate_heads(\n",
    "        context_layer,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "    )\n",
    "\n",
    "    self_output = ttnn.linear(\n",
    "        context_layer,\n",
    "        self_output_weight,\n",
    "        bias=self_output_bias,\n",
    "        memory_config=ttnn.L1_MEMORY_CONFIG,\n",
    "        dtype=ttnn.bfloat16,\n",
    "        core_grid=(batch_size, num_cores_x),\n",
    "    )\n",
    "    ttnn.deallocate(context_layer)\n",
    "\n",
    "    return self_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0c981-a28d-4932-be29-99bbb2d2ed12",
   "metadata": {},
   "source": [
    "## Pre-process the parameters of the optimized model\n",
    "\n",
    "1. Fuse QKV weights and biases\n",
    "2. Reshape and tilize for the optimized operations using preprocess_linear_weight and preprocess_linear_bias\n",
    "3. Move to device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f690cdb-96bf-42b8-afc1-ecc033ca25e5",
   "metadata": {},
   "source": [
    "## Fuse QKV weights and biases \n",
    "- One optimization step is replacing the convnetional 3 Linear OPs (of multiplying the input embeddings by each of Q,K,V weights) by one large fused Linear OP (multiplying the input embeddings by the fused QKV weight tensor)\n",
    "- For the following prpeocessing functions, the main step is from_torch(), followed by tile_layout processing, to eliminate the redundant intemrediate tiling steps between OPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cc736-7d25-4fbc-9b8b-3e7966e0b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttnn.model_preprocessing import (\n",
    "    preprocess_linear_bias,\n",
    "    preprocess_linear_weight,\n",
    ")\n",
    "\n",
    "torch_qkv_weight = torch.cat([torch_query_weight, torch_key_weight, torch_value_weight], dim=-1)\n",
    "torch_qkv_bias = torch.cat([torch_query_bias, torch_key_bias, torch_value_bias], dim=-1)\n",
    "\n",
    "qkv_weight = preprocess_linear_weight(torch_qkv_weight.T, dtype=ttnn.bfloat16)\n",
    "qkv_bias = preprocess_linear_bias(torch_qkv_bias, dtype=ttnn.bfloat16)\n",
    "output_weight = preprocess_linear_weight(torch_output_weight.T, dtype=ttnn.bfloat16)\n",
    "output_bias = preprocess_linear_bias(torch_output_bias, dtype=ttnn.bfloat16)\n",
    "\n",
    "qkv_weight = ttnn.to_device(qkv_weight, device)\n",
    "qkv_bias = ttnn.to_device(qkv_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "output_weight = ttnn.to_device(output_weight, device)\n",
    "output_bias = ttnn.to_device(output_bias, device, memory_config=ttnn.L1_MEMORY_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689248f8-4502-463c-8016-ce3a7149046a",
   "metadata": {},
   "source": [
    "## Run the first iteration of the optimized Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09207576-9166-4b04-97e9-ae7cb3f981f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hidden_states = ttnn.to_layout(hidden_states, ttnn.TILE_LAYOUT)\n",
    "optimized_output = optimized_multi_head_attention(\n",
    "    hidden_states,\n",
    "    qkv_weight,\n",
    "    qkv_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "end = time.time()\n",
    "duration = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade34f2c-0ce4-46fb-84aa-3d1259b7794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimized multi-head attention ran in {duration} seconds for the first iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19005502-261f-4547-b240-7dd014237cc6",
   "metadata": {},
   "source": [
    "## Run a subsequent iteration of the optimized Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46664e-102c-4eb3-a68b-2b7cf6c87b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "optimized_output = optimized_multi_head_attention(\n",
    "    hidden_states,\n",
    "    qkv_weight,\n",
    "    qkv_bias,\n",
    "    output_weight,\n",
    "    output_bias,\n",
    "    num_heads=num_heads,\n",
    ")\n",
    "end = time.time()\n",
    "duration = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61310a-e7dc-4b36-befd-7eb9d7e3c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimized multi-head attention ran in {duration} seconds for the subsequent iteration because of the program cache\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6674b2e-a071-4d0c-a724-0fd803526262",
   "metadata": {},
   "source": [
    "Note that the optimized multi-head attention is 2 orders of magnitude faster than the initial version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e96ae-7440-459b-b8cf-52555d147399",
   "metadata": {},
   "source": [
    "## Check that the output of the optimized version matches the output of the original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58887ac-e9ed-4af2-a20e-26cf741f3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_output = ttnn.to_torch(output)\n",
    "torch_optimized_output = ttnn.to_torch(optimized_output)\n",
    "\n",
    "assert torch.allclose(torch_output, torch_optimized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb918b-52b2-42e9-a613-f5cfa6296162",
   "metadata": {},
   "source": [
    "## Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326205fb-7f60-46f3-88fe-85a3da1140d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.close(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
