name: "Device perf regressions and output report"

on:
  workflow_dispatch:
  schedule:
    - cron: "0 2 * * *"
  workflow_call:

jobs:
  device-perf:
    strategy:
      # Do not fail-fast because we need to ensure all tests go to completion
      # so we try not to get hanging machines
      fail-fast: false
      matrix:
        runner-info: [
          {arch: grayskull, runs-on: ["perf-no-reset-grayskull-e04cs07", "self-reset"], machine-type: "bare_metal"},
        ]
        test-group: [
          {name: resnet, cmd: source tests/scripts/run_performance.sh --pipeline-type device_performance_bare_metal --tt-arch grayskull; run_device_perf_resnet},
          {name: bert, cmd: source tests/scripts/run_performance.sh --pipeline-type device_performance_bare_metal --tt-arch grayskull; run_device_perf_bert},
        ]
    name: ${{ matrix.test-group.name }} ${{ matrix.runner-info.arch }} ${{ matrix.runner-info.machine-type }}
    env:
      TT_METAL_ENV: ${{ vars.TT_METAL_ENV }}
      ARCH_NAME: ${{ matrix.runner-info.arch }}
      CONFIG: ci
    environment: dev
    runs-on: ${{ matrix.runner-info.runs-on }}
    steps:
      - uses: tenstorrent-metal/metal-workflows/.github/actions/checkout-with-submodule-lfs@v2.0.0
      - name: Ensure weka mount is active
        run: |
          sudo systemctl restart mnt-MLPerf.mount
          sudo /etc/rc.local
          ls -al /mnt/MLPerf/bit_error_tests
      - name: Set up dynamic env vars for build
        run: |
          echo "TT_METAL_HOME=$(pwd)" >> $GITHUB_ENV
      - name: Build tt-metal and libs
        run: |
          ./scripts/build_scripts/build_with_profiler_opt.sh
      - name: Run device performance regressions
        timeout-minutes: 30
        run: |
          source build/python_env/bin/activate
          ${{ matrix.test-group.cmd }}
      - name: Check device perf
        id: check-device-perf-report
        if: ${{ !cancelled() }}
        run: |
          env python models/perf/merge_device_perf_results.py
          ls -hal
          export DEVICE_PERF_REPORT_FILENAME=Models_Device_Perf_${{ matrix.test-group.name }}_$(date +%Y_%m_%d).csv
          ls -hal $DEVICE_PERF_REPORT_FILENAME
          echo "device_perf_report_filename=$DEVICE_PERF_REPORT_FILENAME" >> "$GITHUB_OUTPUT"
      - name: Upload device perf report
        if: ${{ !cancelled() && steps.check-device-perf-report.conclusion == 'success' }}
        uses: actions/upload-artifact@v4
        with:
          name: device-perf-report-csv-${{ matrix.test-group.name }}-${{ matrix.runner-info.arch }}-${{ matrix.runner-info.machine-type }}
          path: "${{ steps.check-device-perf-report.outputs.device_perf_report_filename }}"
